# {{ ansible_managed }}
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName={{ role_slurm_cluster_name }}
SlurmctldHost={{ role_slurm_control_host }}
#ControlAddr=
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
SlurmctldParameters=enable_configless
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/var/spool/slurm
SlurmdSpoolDir=/var/spool/slurm
SwitchType=switch/none
# workaround for https://jira.vbc.ac.at/browse/COPS-22
MpiDefault=pmi2
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/cgroup
#PluginDir=
#FirstJobId=
# Want nodes that drop out of SLURM's configuration to be automatically
# returned to service when they come back.
ReturnToService=2
MaxArraySize={{ role_slurm_max_array_size }}
MaxJobCount={{ role_slurm_max_job_count }}
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
PropagateResourceLimitsExcept=MEMLOCK
UnkillableStepTimeout=180
#Prolog=
PrologFlags=contain,alloc,x11
X11Parameters=home_xauthority
RebootProgram="{{ role_slurm_reboot_program }}"
ScronParameters=enable
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
# best to combine affinity and cgroup plugin according to https://slurm.schedmd.com/slurm.conf.html
TaskPlugin=task/affinity,task/cgroup
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
# required for polyinstantiated temp
UsePAM=1
{% if role_slurm_healthcheck_program != "" %}
HealthCheckInterval={{ role_slurm_healthcheck_interval }}
HealthCheckProgram={{ role_slurm_healthcheck_program }}
{% endif %}
GresTypes=gpu
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
{% if role_slurm_conf_scheduler_parameters %}
SchedulerParameters={{ role_slurm_conf_scheduler_parameters }}
{% endif %}
#
#SchedulerAuth=
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
PriorityType=priority/multifactor
PriorityDecayHalfLife={{ role_slurm_prio_decay_half_time }}
{% if role_slurm_prio_usage_period %}
PriorityUsageResetPeriod={{ role_slurm_prio_usage_period }}
{% endif %}
PriorityCalcPeriod={{role_slurm_prio_calc_period}}
PriorityWeightAge={{ role_slurm_prio_weight_age }}
PriorityWeightFairshare={{ role_slurm_prio_weight_fairshare }}
PriorityWeightJobSize={{ role_slurm_prio_weight_jobsize }}
PriorityWeightPartition={{ role_slurm_prio_weight_partition }}
PriorityWeightQOS={{ role_slurm_prio_weight_qos }}
PriorityMaxAge={{ role_slurm_prio_max_age }}
{% if role_slurm_prio_weight_tres %}
PriorityWeightTRES={{ role_slurm_prio_weight_tres }}
{% endif %}
PriorityFavorSmall={{ role_slurm_prio_favor_small }}
PreemptType=preempt/none
PreemptMode=OFF
#
# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmctldSyslogDebug=info
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmdSyslogDebug=info
JobCompType=jobcomp/filetxt
JobCompLoc=/var/log/slurm/slurm_jobcomp.log
{% if role_slurm_conf_debug_flags %}
DebugFlags={{ role_slurm_conf_debug_flags }}
{% endif %}

#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/cgroup
JobAcctGatherFrequency=30
{% if role_slurm_db_host is defined %}
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{ role_slurm_db_host }}
{% else %}
AccountingStorageType=accounting_storage/filetxt
{% endif %}
AccountingStorageEnforce=qos,limits
{% if role_slurm_accounting_storage_tres | length > 0 %}
AccountingStorageTRES={{ role_slurm_accounting_storage_tres | unique | join (',') }}
{% endif %}
AcctGatherEnergyType=acct_gather_energy/none
AcctGatherInfinibandType=acct_gather_infiniband/none
AcctGatherFilesystemType=acct_gather_filesystem/none
AcctGatherProfileType=acct_gather_profile/none

#JOB SUBMIT PLUGIN
{% if role_slurm_job_submit_plugin %}
JobSubmitPlugins=lua
{% endif %}
# CHECKPOINTING
CheckpointType=checkpoint/none


#
# COMPUTE NODES
{% set nodes_list = [] %}
{% for part in role_slurm_partitions %}
{% for group in part.get('groups', [part]) %}
{% if (group.num_nodes | int) > 0 and group.name not in nodes_list %}
{{ nodes_list.append(group.name) }}
NodeName={{group.cluster_name|default(role_slurm_node_prefix)}}-{{group.name}}-[0-{{ (group.num_nodes | int)-1}}] \
{% set group_name = group.cluster_name|default(role_slurm_cluster_name) ~ '_' ~ group.name %}
{# due to https://github.com/ansible/ansible/issues/66945 we need to check which hosts are really reachable #}
{% set active_hosts = hostvars | dict2items | json_query('[?value.ansible_processor_cores].key') %}
{# If using --limit, the first host in each group may not have facts available. Find one that does. #}
{% set group_hosts = groups[group_name] | intersect(active_hosts) %}
{% if group_hosts | length > 0 %}
{% set first_host_hv = hostvars[group_hosts | first] %}
{% set gpu_info = first_host_hv.ansible_local.gpus | default({'found': false, 'count': 0}) %}
{% if 'ram_mb' in group %}
    RealMemory={{ group.ram_mb }} \
{% else %}
    RealMemory={{ first_host_hv.ansible_memory_mb.real.total }} \
{% endif %}
    Procs={{ (first_host_hv['ansible_processor_count'] | int) * (first_host_hv['ansible_processor_cores'] | int) }} \
    Sockets={{first_host_hv['ansible_processor_count']}} \
    CoresPerSocket={{first_host_hv['ansible_processor_cores']}} \
    ThreadsPerCore={{first_host_hv['ansible_processor_threads_per_core']}} \
    {% if 'features' in group %}Features={{ group.features }}{% endif %} \
    {% if 'weight' in group %}Weight={{ group.weight }}{% endif %} \
    {% if gpu_info.found | bool and gpu_info.count > 0 %}Gres=gpu:{{ gpu_info['type'] }}:{{ gpu_info['count'] }}{% endif %} \
{% endif %}
    State=UNKNOWN
{% endif %}
{% endfor %}
{% endfor %}
{% if not role_slurm_default_partition  and role_slurm_partitions | length %}
{% set role_slurm_default_partition = (role_slurm_partitions | first).name %}
{% endif %}
{% for part in role_slurm_partitions %}
{% if not part.hidden is defined or not (part.hidden | bool) %}
PartitionName={{part.name}} Nodes="{% for group in part.get('groups', [part]) %}{% if (group.num_nodes |int) > 0 %}{{group.cluster_name|default(role_slurm_node_prefix)}}-{{group.name}}-[{{ group.min_ix | default(0) }}-{{ group.max_ix | default( (group.num_nodes | int)) -1 }}]{% if not loop.last %},{% endif %}{% endif %}{% endfor %}" DefMemPerCPU={{ role_slurm_default_partition_mem }} Default={{ (role_slurm_default_partition == part.name) | ternary('YES', 'NO') }} {%if role_slurm_max_job_time %}MaxTime={{ role_slurm_max_job_time }}{% endif %} {%if role_slurm_job_def_time %}DefaultTime={{ role_slurm_job_def_time }}{% endif %} {% if part.qos is defined and part.qos | length > 0 %}{% if not part.qos_deny | default(False) %}AllowQos={{ part.qos | join(',') }}{% endif %} {% if part.qos_deny | default(False) %}DenyQos={{ part.qos | join(',') }}{% endif %}{% endif %} {% if part.allow_groups | default(False) %}AllowGroups={{ part.allow_groups | join(',') }}{% endif %} State=UP OverSubscribe=NO
{% endif %}
{% endfor %}
